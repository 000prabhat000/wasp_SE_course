
\documentclass[12pt,a4paper]{article}

\usepackage[english]{babel}
\usepackage[a4paper,top=1cm,bottom=2cm,left=2cm,right=2cm,marginparwidth=1.75cm]{geometry}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=black]{hyperref}
\usepackage[sc]{titlesec}
\usepackage{textcomp}

%\titleformat{\subsection}[runin]
%{\sc}{\thesubsection}{1em}{}

\title{%
  WASP Assignment 2 \\~\\
  \textsc{Software Engineering}\\~\\
  \small\textsc{Umeå Universitet}}
\author{Minal Suresh Patil}
\date{}

\begin{document}
\maketitle
\begin{abstract}
In a legal framework, information about consumers is often separated into authorised features that can be used as input to company choices, such as income and assets, and protected attributes, such as gender and race. The issue is that a protected group of people may be disproportionately harmed by an algorithm's unjust or biased resource allocation, such as access to credit. Simply removing protected attributes from the data (fairness through unawareness) is not only insufficient, but also detrimental in the fight against algorithmic bias. Even after eliminating protected indicators, there is still a possibility of algorithmic prejudice through triangulation, leading to, for instance, unfair mortgage allocation results. My research is focused on investigating techniques that facilitate model explainability which can be divided into two categories, ex-ante and ex-post. Ex-ante explainability, as the name suggests, refers to decisions made by the model developer before the model is actually implemented. This covers the straightforward strategy of picking a model that is innately interpretable. Several characteristics constitute an interpretable model. Examples include additivity (can outcomes be expressed as a weighted sum of inputs), linearity (is the influence of a feature proportional to the feature's value), and visualisability (can the impact of model features be simply displayed in a graph).

\end{abstract}

\begin{itemize}
    \item \textsc{Human-Computer Interaction:}
    The study of computer technology known as HCI concentrates on the interface between people and computers. Given that computers were only created recently, the phrase is relatively new, having been used for the first time in the 1970s or 1980s. ACM describes HCI as "a field concerned with the design, assessment, and implementation of interactive computer systems for human use and with the study of key phenomena surrounding them."
As a result, HCI is extremely interdisciplinary and includes elements from a wide range of disciplines, including computer graphics, virtual reality, artificial intelligence, human factors, cognitive psychology, neurology, and many more.
A keyboard, joystick or gaming pad, another interactive device, a microphone, and other modalities are among the many options for communicating with a computer. The majority of apps employ some sort of graphical user interface (GUI), but with the introduction of voice user interfaces (VUI), like Siri and Alexa, which make use of sophisticated speech recognition technologies, other techniques are gaining popularity. In my ressearch
    \item \textsc{Regulations and Compliance:}
    In recent years, lawmakers have started including explicit criteria for explainability into the legal frameworks of many countries, and regulators have been more and more focused on compliance concerns that occur in the context of AI and ML models. For instance, US banking authorities have given possible discrimination against protected classes in commercial operations like consumer lending a great deal of attention.
According to the Equal Credit Opportunity Act (ECOA), creditors must provide clients explicit explanations for any adverse actions they take, such denying them credit. The US Consumer Financial Protection Bureau (CFPB) recently stated that it is "particularly interested in exploring the accuracy of explainability methods, particularly as applied to deep learning and other complex ensemble models" and how to convey the primary reasons in a way that accurately reflects the factors.
    \item \textsc{Requirements Engineering}
The relevance of software requirements engineering (RE) is emphasized by the fact that it is a phase in which the stakeholders have massive impact over the achieving of the software project, and the decisions made during this phase customarily have a significant, frequently irreversible effect on the following phases. RE has evolved over the past half - century more than from being a mostly technically oriented undertaking that dealt with mathematical problems to a context-, anthropologically and socially sensitive discipline that dealt with poorly structured problems such complex challenges. This modification invariably has an impact on how software requirements engineering pedagogy should be understood, planned, and pursued, as well as on the expected role and ideal competencies of a software requirements engineer. Teaching needs serves as the key driver while behind RE notions (interspersed and connected actions in a RE process and, presumably, artefacts resulting from those activities). They could be categorized into basic concepts (abstract, tied directly to a RE process) and auxiliary notions (part of a RE process indirectly to support one or more primary concepts, and concrete). 
The principal RE principles are active learning, teamwork, creation, discussion, empathy, enjoyable learning, ensuring the semiotic quality of software requirements (by addressing ambiguities, inconsistencies, and indeterminacies), group learning, team building, incrementing, iterating, negotiating, planning, problem solving, reading aloud, user testing, and writing.
\end{itemize}

\noindent \textsc{Future trends in Software Engineering in AI-ML} \\
Information retrieval and data science go in tandem. It is reasonable to say that the results of data science activity provide as input for the process of knowledge discovery. Data scientists are likely to be in high demand in the United States alone by the start of the next year 2023 as the field continues to develop as a distinct discipline. Internationally, more of it is required if we take into consideration the interests of the remainder of the world. As a corollary, individuals are now being rapidly offered data science courses from both private and public educational institutions. Naturally, utilizing data mining and machine learning tools and techniques is where the courses' general emphasis resides. We are cognizant that the methodology for the data science field makes it unavoidable for a data scientist's work to end up as a piece of software. culminating into a products that would produce reports to use for decision-making by company executives and managers. In this perspective, once something is converted into software, it has value for it's own owners, or the clients who contracted the project, and therefore becomes a "asset" if you prefer. The reality that the product takes the form of code means that the software engineering field may make a significant contribution to the code's long-term merits. We must not, under any circumstances, indicate that data science and machine learning reduces to software engineering. We are not lobbying for them to possess data science knowledge or be a software engineer. Alternatively, we are suggesting that data science aspirants be taught basics of software engineering in a simple but significant manner. We are advocating for a discourse on software engineering basics, if not in a very substantial way, so that at least sufficient for the data science to verify sure that the final software delivered is trustworthy and maintainable.




%\bibliographystyle{plain}
%\bibliography{sample}
\end{document}